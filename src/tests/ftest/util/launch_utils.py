"""
  (C) Copyright 2022-2023 Intel Corporation.

  SPDX-License-Identifier: BSD-2-Clause-Patent
"""
from logging import getLogger
import os
import re
import sys
import time

from collection_utils import collect_test_result, TEST_RESULTS_DIRS
from data_utils import list_unique, list_flatten, dict_extract_values
from host_utils import get_node_set, get_local_host, HostInfo, HostException
from results_utils import LaunchTestName
from run_utils import RunException, run_local, run_remote
from slurm_utils import show_partition, create_partition, delete_partition
from test_env_utils import TestEnvironment
from user_utils import groupadd, useradd, userdel, get_group_id, get_user_groups
from yaml_utils import get_yaml_data

logger = getLogger()


class LaunchException(Exception):
    """Exception for launch.py execution."""


def fault_injection_enabled():
    """Determine if fault injection is enabled.

    Returns:
        bool: whether or not fault injection is enabled

    """
    logger.debug("Checking for fault injection enablement via 'fault_status':")
    try:
        run_local("fault_status", check=True)
        logger.debug("  Fault injection is enabled")
        return True
    except RunException:
        # Command failed or yielded a non-zero return status
        logger.debug("  Fault injection is disabled")
    return False


class AvocadoInfo():
    """Information about this version of avocado."""

    def __init__(self):
        """Initialize an AvocadoInfo object."""
        self.major = 0
        self.minor = 0

    def __str__(self):
        """Get the avocado version as a string.

        Returns:
            str: the avocado version

        """
        return f"Avocado {str(self.major)}.{str(self.minor)}"

    @staticmethod
    def set_config(stage, overwrite=False):
        """Set up the avocado config files if they do not already exist.

        Should be called before get_setting() to ensure any files generated by this method are
        included.

        Args:
            stage (str): name of the stage to use in the job results path
            overwrite (bool, optional): if true overwrite any existing avocado config files. If
                false do not modify any existing avocado config files. Defaults to False.

        Raises:
            LaunchException: if there is an error writing an avocado config file

        """
        stage_name = ' '.join(stage.split('_'))
        daos_base = os.getenv("DAOS_BASE", None)
        logs_dir = os.path.expanduser("~")
        if os.getenv("TEST_RPMS", "false").lower() == "true":
            logs_dir = os.path.join(os.sep, "var", "tmp", "ftest")
        elif daos_base:
            logs_dir = os.path.join(daos_base, "install", "lib", "daos", "TESTING", "ftest")

        job_results_dir = os.path.join(logs_dir, "avocado", "job-results", stage_name)
        data_dir = os.path.join(logs_dir, "avocado", "data", stage_name)
        config_dir = os.path.join(
            os.environ.get("VIRTUAL_ENV", os.path.expanduser("~")), ".config", "avocado")
        config_file = os.path.join(config_dir, "avocado.conf")
        sysinfo_dir = os.path.join(config_dir, "sysinfo")
        sysinfo_files_file = os.path.join(sysinfo_dir, "files")
        sysinfo_commands_file = os.path.join(sysinfo_dir, "commands")

        # Create the avocado configuration directories
        os.makedirs(config_dir, exist_ok=True)
        os.makedirs(sysinfo_dir, exist_ok=True)

        # Create the avocado config file. If one exists do not overwrite it.
        if not os.path.exists(config_file) or overwrite:
            # Give the avocado test tearDown method a minimum of 120 seconds to complete when the
            # test process has timed out.  The test harness will increment this timeout based upon
            # the number of pools created in the test to account for pool destroy command timeouts.
            config = [
                "[datadir.paths]\n",
                f"logs_dir = {job_results_dir}\n",
                f"data_dir = {data_dir}\n",
                "\n",
                "[job.output]\n",
                "loglevel = DEBUG\n",
                "\n",
                "[runner.timeout]\n",
                "after_interrupted = 120\n",
                "process_alive = 120\n",
                "process_died = 120\n",
                "\n",
                "[sysinfo.collectibles]\n",
                f"files = {sysinfo_files_file}\n",
                f"commands = {sysinfo_commands_file}\n",
            ]

            try:
                with open(config_file, "w", encoding="utf-8") as config_handle:
                    config_handle.writelines(config)
            except IOError as error:
                raise LaunchException(f"Error writing avocado config file {config_file}") from error

        # Create the avocado system info files file. If one exists do not overwrite it.
        if not os.path.exists(sysinfo_files_file) or overwrite:
            try:
                with open(sysinfo_files_file, "w", encoding="utf-8") as sysinfo_files_handle:
                    sysinfo_files_handle.write("/proc/mounts\n")
            except IOError as error:
                raise LaunchException(
                    f"Error writing avocado config file {sysinfo_files_file}") from error

        # Create the avocado system info commands file. If one exists do not overwrite it.
        if not os.path.exists(sysinfo_commands_file) or overwrite:
            try:
                with open(sysinfo_commands_file, "w", encoding="utf-8") as sysinfo_commands_handle:
                    sysinfo_commands_handle.write("ps axf\n")
                    sysinfo_commands_handle.write("dmesg\n")
                    sysinfo_commands_handle.write("df -h\n")
            except IOError as error:
                raise LaunchException(
                    f"Error writing avocado config file {sysinfo_commands_file}") from error

    def set_version(self):
        """Set the avocado major and minor versions.

        Raises:
            LaunchException: if there is an error running 'avocado -v'

        """
        try:
            # pylint: disable=import-outside-toplevel
            from avocado.core.version import MAJOR, MINOR
            self.major = int(MAJOR)
            self.minor = int(MINOR)

        except ModuleNotFoundError:
            # Once lightweight runs are using python3-avocado, this can be removed
            try:
                result = run_local("avocado -v", check=True)
            except RunException as error:
                message = "Error obtaining avocado version after failed avocado.core.version import"
                raise LaunchException(message) from error
            try:
                version = re.findall(r"(\d+)\.(\d+)", result.stdout)[0]
                self.major = int(version[0])
                self.minor = int(version[1])
            except IndexError as error:
                raise LaunchException("Error extracting avocado version from command") from error

    @staticmethod
    def get_setting(section, key, default=None):
        """Get the value for the specified avocado setting.

        Args:
            section (str): avocado setting section name
            key (str): avocado setting key name
            default (object): default value to use if setting is undefined

        Raises:
            RunException: if there is an error getting the setting from the avocado command

        Returns:
            object: value for the avocado setting or None if not defined

        """
        try:
            # pylint: disable=import-outside-toplevel
            from avocado.core.settings import settings, SettingsError
            try:
                # Newer versions of avocado use this approach
                config = settings.as_dict()
                return config.get(".".join([section, key]))

            except AttributeError:
                # Older version of avocado, like 69LTS, use a different method
                # pylint: disable=no-member
                try:
                    return settings.get_value(section, key)
                except SettingsError:
                    # Setting not found
                    pass

            except KeyError:
                # Setting not found
                pass

        except ModuleNotFoundError:
            # Once lightweight runs are using python3-avocado, this can be removed
            result = run_local("avocado config", check=True)
            try:
                return re.findall(rf"{section}\.{key}\s+(.*)", result.stdout)[0]
            except IndexError:
                # Setting not found
                pass

        return default

    def get_logs_dir(self):
        """Get the avocado directory in which the test results are stored.

        Returns:
            str: the directory used by avocado to log test results

        """
        default_base_dir = os.path.join("~", "avocado", "job-results")
        return os.path.expanduser(self.get_setting("datadir.paths", "logs_dir", default_base_dir))

    def get_directory(self, directory, create=True):
        """Get the avocado test directory for the test.

        Args:
            directory (str): name of the sub directory to add to the logs directory
            create (bool, optional): whether or not to create the directory if it doesn't exist.
                Defaults to True.

        Returns:
            str: the directory used by avocado to log test results

        """
        logs_dir = self.get_logs_dir()
        test_dir = os.path.join(logs_dir, directory)
        if create:
            os.makedirs(test_dir, exist_ok=True)
        return test_dir

    def get_list_command(self):
        """Get the avocado list command for this version of avocado.

        Returns:
            list: avocado list command parts

        """
        if self.major >= 83:
            return ["avocado", "list"]
        if self.major >= 82:
            return ["avocado", "--paginator=off", "list"]
        return ["avocado", "list", "--paginator=off"]

    def get_list_regex(self):
        """Get the regular expression used to get the test file from the avocado list command.

        Returns:
            str: regular expression to use to get the test file from the avocado list command output

        """
        if self.major >= 92:
            return r"avocado-instrumented\s+(.*):"
        return r"INSTRUMENTED\s+(.*):"

    def get_run_command(self, test, tag_filters, sparse, failfast):
        """Get the avocado run command for this version of avocado.

        Args:
            test (TestInfo): the test information
            tag_filters (list): optional '--filter-by-tags' arguments
            sparse (bool): whether or not to provide sparse output of the test execution
            failfast (bool): whether or not to fail fast

        Returns:
            list: avocado run command

        """
        command = ["avocado"]
        if not sparse and self.major >= 82:
            command.append("--show=test")
        command.append("run")
        if self.major >= 82:
            command.append("--ignore-missing-references")
        else:
            command.extend(["--ignore-missing-references", "on"])
        if self.major >= 83:
            command.append("--disable-tap-job-result")
        else:
            command.extend(["--html-job-result", "on"])
            command.extend(["--tap-job-result", "off"])
        if not sparse and self.major < 82:
            command.append("--show-job-log")
        if tag_filters:
            command.extend(tag_filters)
        if failfast:
            command.extend(["--failfast", "on"])
        command.extend(["--mux-yaml", test.yaml_file])
        if test.extra_yaml:
            command.extend(test.extra_yaml)
        command.extend(["--", str(test)])
        return command


class TestInfo():
    """Defines the python test file and its associated test yaml file."""

    YAML_INFO_KEYS = [
        "test_servers",
        "server_partition",
        "server_reservation",
        "test_clients",
        "client_partition",
        "client_reservation",
        "client_users",
    ]

    def __init__(self, test_file, order, yaml_extension=None):
        """Initialize a TestInfo object.

        Args:
            test_file (str): the test python file
            order (int): order in which this test is executed
            yaml_extension (str, optional): if defined and a test yaml file exists with this
                extension, the yaml file will be used in place of the default test yaml file.
        """
        self.name = LaunchTestName(test_file, order, 0)
        self.test_file = test_file
        self.yaml_file = ".".join([os.path.splitext(self.test_file)[0], "yaml"])
        if yaml_extension:
            custom_yaml = ".".join(
                [os.path.splitext(self.test_file)[0], str(yaml_extension), "yaml"])
            if os.path.exists(custom_yaml):
                self.yaml_file = custom_yaml
        parts = self.test_file.split(os.path.sep)[1:]
        self.python_file = parts.pop()
        self.directory = os.path.join(*parts)
        self.class_name = f"FTEST_launch.{self.directory}-{os.path.splitext(self.python_file)[0]}"
        self.host_info = HostInfo()
        self.yaml_info = {}
        self.extra_yaml = []

    def __str__(self):
        """Get the test file as a string.

        Returns:
            str: the test file

        """
        return self.test_file

    def set_yaml_info(self, include_local_host=False):
        """Set the test yaml data from the test yaml file.

        Args:
            include_local_host (bool, optional): whether or not the local host be included in the
                set of client hosts. Defaults to False.
        """
        self.yaml_info = {"include_local_host": include_local_host}
        yaml_data = get_yaml_data(self.yaml_file)
        info = {}
        for key in self.YAML_INFO_KEYS:
            # Get the unique values with lists flattened
            values = list_unique(list_flatten(dict_extract_values(yaml_data, [key], (str, list))))
            if values:
                # Use single value if list only contains 1 element
                info[key] = values if len(values) > 1 else values[0]

        logger.debug("Test yaml information for %s:", self.test_file)
        for key in self.YAML_INFO_KEYS:
            if key in (self.YAML_INFO_KEYS[0], self.YAML_INFO_KEYS[3]):
                self.yaml_info[key] = get_node_set(info[key] if key in info else None)
            else:
                self.yaml_info[key] = info[key] if key in info else None
            logger.debug("  %-18s = %s", key, self.yaml_info[key])

    def set_host_info(self, control_node):
        """Set the test host information using the test yaml file.

        Args:
            control_node (NodeSet): the slurm control node

        Raises:
            LaunchException: if there is an error getting the host from the test yaml or a problem
            setting up a slum partition

        """
        logger.debug("Using %s to define host information", self.yaml_file)
        if self.yaml_info["include_local_host"]:
            logger.debug("  Adding the localhost to the clients: %s", get_local_host())
        try:
            self.host_info.set_hosts(
                control_node, self.yaml_info[self.YAML_INFO_KEYS[0]],
                self.yaml_info[self.YAML_INFO_KEYS[1]], self.yaml_info[self.YAML_INFO_KEYS[2]],
                self.yaml_info[self.YAML_INFO_KEYS[3]], self.yaml_info[self.YAML_INFO_KEYS[4]],
                self.yaml_info[self.YAML_INFO_KEYS[5]], self.yaml_info["include_local_host"])
        except HostException as error:
            raise LaunchException("Error getting hosts from {self.yaml_file}") from error

    def get_yaml_client_users(self):
        """Find all the users in the specified yaml file.

        Returns:
            list: list of (user, group) to create

        """
        yaml_data = get_yaml_data(self.yaml_file)
        return list_flatten(dict_extract_values(yaml_data, ["client_users"], list))

    def get_log_file(self, logs_dir, repeat, total):
        """Get the test log file name.

        Args:
            logs_dir (str): base directory in which to place the log file
            repeat (int): current test repetition
            total (int): total number of test repetitions

        Returns:
            str: a test log file name composed of the test class, name, and optional repeat count

        """
        name = os.path.splitext(self.python_file)[0]
        log_file = f"{self.name.order_str}-{self.directory}-{name}-launch.log"
        if total > 1:
            self.name.repeat = repeat
            os.makedirs(os.path.join(logs_dir, self.name.repeat_str), exist_ok=True)
            return os.path.join(logs_dir, self.name.repeat_str, log_file)
        return os.path.join(logs_dir, log_file)


class TestRunner():
    """."""

    def __init__(self, avocado, launch_result, total_tests, total_repeats, tag_filters):
        """Initialize a FunctionalTest object.

        Args:
            avocado (AvocadoInfo): foo
            result (TestResult): _description_
        """
        self.avocado = avocado
        self.launch_result = launch_result
        self.test_result = None
        self.total_tests = total_tests
        self.total_repeats = total_repeats
        self.tag_filters = tag_filters
        self.local_host = get_local_host()

    def prepare(self, test_log_file, test, repeat, user_create, slurm_setup, control_host,
                partition_hosts):
        """Prepare the test for execution.

        Args:
            test (TestInfo): the test information
            repeat (str): the test repetition sequence, e.g. '1/10'
            user_create (bool): whether to create extra test users defined by the test

        Returns:
            int: status code: 0 = success, 128 = failure

        """
        logger.debug("=" * 80)
        logger.info(
            "Preparing to run the %s test on repeat %s/%s", test, repeat, self.total_repeats)

        # Create a new TestResult for this test
        self.test_result = self.launch_result.add_test(
            test.class_name, test.name.copy(), test_log_file)
        self.test_result.start()

        # Setup the test host information, including creating any required slurm partitions
        status = self._setup_host_information(test, slurm_setup, control_host, partition_hosts)
        if status:
            return status

        # Setup (remove/create/list) the common test directory on each test host
        status = self._setup_test_directory(test)
        if status:
            return status

        # Setup additional test users
        status = self._user_setup(test, user_create)
        if status:
            return status

        # Generate certificate files for the test
        return self._generate_certs()

    def execute(self, test, repeat, number, sparse, fail_fast):
        """Run the specified test.

        Args:
            test (TestInfo): the test information
            repeat (int): the test repetition number
            number (int): the test sequence number in this repetition
            sparse (bool): whether to use avocado sparse output
            fail_fast(bool): whether to use the avocado fail fast option

        Returns:
            int: status code: 0 = success, >0 = failure

        """
        # Avoid counting the test execution time as part of the processing time of this test
        self.test_result.end()

        logger.debug("=" * 80)
        command = self.avocado.get_run_command(test, self.tag_filters, sparse, fail_fast)
        logger.info(
            "[Test %s/%s] Running the %s test on repetition %s/%s",
            number, self.total_tests, test, repeat, self.total_repeats)
        start_time = int(time.time())

        try:
            return_code = run_local(" ".join(command), capture_output=False, check=False).returncode
            if return_code == 0:
                logger.debug("All avocado test variants passed")
            elif return_code & 2 == 2:
                logger.debug("At least one avocado test variant failed")
            elif return_code & 4 == 4:
                message = "Failed avocado commands detected"
                self.test_result.fail_test("Execute", message)
            elif return_code & 8 == 8:
                logger.debug("At least one avocado test variant was interrupted")
            if return_code:
                self._collect_crash_files()

        except RunException:
            message = f"Error executing {test} on repeat {repeat}"
            self.test_result.fail_test("Execute", message, sys.exc_info())
            return_code = 1

        end_time = int(time.time())
        logger.info("Total test time: %ss", end_time - start_time)
        return return_code

    def process(self, job_results_dir, test, repeat, stop_daos, archive, rename,
                jenkins_xml, core_files, threshold):
        """Process the test results.

        This may include (depending upon argument values):
            - Stopping any running servers or agents
            - Resetting the server storage
            - Archiving any files generated by the test and including them with the test results
            - Renaming the test results directory and results.xml entries
            - Processing any core files generated by the test

        Args:
            test (TestInfo): the test information
            repeat (int): the test repetition number
            stop_daos (bool): whether or not to stop daos servers/clients after the test
            archive (bool): whether or not to collect remote files generated by the test
            rename (bool): whether or not to rename the default avocado job-results directory names
            jenkins_xml (bool): whether or not to update the results.xml to use Jenkins-style names
            core_files (dict): location and pattern defining where core files may be written
            threshold (str): optional upper size limit for test log files

        Returns:
            int: status code: 0 = success, >0 = failure

        """
        # Mark the continuation of the processing of this test
        self.test_result.start()

        logger.debug("=" * 80)
        logger.info(
            "Processing the %s test after the run on repeat %s/%s",
            test, repeat, self.total_repeats)
        status = collect_test_result(
            test, self.test_result, job_results_dir, stop_daos, archive, rename, jenkins_xml,
            core_files, threshold, self.total_repeats)

        # Mark the execution of the test as passed if nothing went wrong
        if self.test_result.status is None:
            self.test_result.pass_test()

        # Mark the end of the processing of this test
        self.test_result.end()

        return status

    def _setup_host_information(self, test, slurm_setup, control_host, partition_hosts):
        """Set up the test host information and any required partitions.

        Args:
            test (TestInfo): the test information
            slurm_setup (bool):
            control_host (NodeSet):
            partition_hosts (NodeSet):

        Returns:
            int: status code: 0 = success, 128 = failure

        """
        logger.debug("-" * 80)
        logger.debug("Setting up host information for %s", test)

        # Verify any required partitions exist
        if test.yaml_info["client_partition"]:
            partition = test.yaml_info["client_partition"]
            logger.debug("Determining if the %s client partition exists", partition)
            exists = show_partition(control_host, partition).passed
            if not exists and not slurm_setup:
                message = f"Error missing {partition} partition"
                self.test_result.fail_test("Prepare", message, None)
                return 128
            if slurm_setup and exists:
                logger.info(
                    "Removing existing %s partition to ensure correct configuration", partition)
                if not delete_partition(control_host, partition).passed:
                    message = f"Error removing existing {partition} partition"
                    self.test_result.fail_test("Prepare", message, None)
                    return 128
            if slurm_setup:
                hosts = partition_hosts.difference(test.yaml_info["test_servers"])
                logger.debug(
                    "Partition hosts from '%s', excluding test servers '%s': %s",
                    partition_hosts, test.yaml_info["test_servers"], hosts)
                if not hosts:
                    message = "Error no partition hosts exist after removing the test servers"
                    self.test_result.fail_test("Prepare", message, None)
                    return 128
                logger.info("Creating the '%s' partition with the '%s' hosts", partition, hosts)
                if not create_partition(control_host, partition, hosts).passed:
                    message = f"Error adding the {partition} partition"
                    self.test_result.fail_test("Prepare", message, None)
                    return 128

        # Define the hosts for this test
        try:
            test.set_host_info(control_host)
        except LaunchException:
            message = "Error setting up host information"
            self.test_result.fail_test("Prepare", message, sys.exc_info())
            return 128

        # Log the test information
        msg_format = "%3s  %-40s  %-60s  %-20s  %-20s"
        logger.debug("-" * 80)
        logger.debug("Test information:")
        logger.debug(msg_format, "UID", "Test", "Yaml File", "Servers", "Clients")
        logger.debug(msg_format, "-" * 3, "-" * 40, "-" * 60, "-" * 20, "-" * 20)
        logger.debug(
            msg_format, test.name.order, test.test_file, test.yaml_file,
            test.host_info.servers.hosts, test.host_info.clients.hosts)
        return 0

    def _setup_test_directory(self, test):
        """Set up the common test directory on all hosts.

        Args:
            test (TestInfo): the test information

        Returns:
            int: status code: 0 = success, 128 = failure

        """
        logger.debug("-" * 80)
        test_env = TestEnvironment()
        hosts = test.host_info.all_hosts
        hosts.add(self.local_host)
        logger.debug("Setting up '%s' on %s:", test_env.log_dir, hosts)
        commands = [
            f"sudo -n rm -fr {test_env.log_dir}",
            f"mkdir -p {test_env.log_dir}",
            f"chmod a+wrx {test_env.log_dir}",
            f"ls -al {test_env.log_dir}",
            f"mkdir -p {test_env.user_dir}"
        ]
        # Predefine the sub directories used to collect the files process()/_archive_files()
        for directory in TEST_RESULTS_DIRS:
            commands.append(f"mkdir -p {test_env.log_dir}/{directory}")
        for command in commands:
            if not run_remote(hosts, command).passed:
                message = "Error setting up the common test directory on all hosts"
                self.test_result.fail_test("Prepare", message, sys.exc_info())
                return 128
        return 0

    def _user_setup(self, test, create=False):
        """Set up test users on client nodes.

        Args:
            test (TestInfo): the test information
            create (bool, optional): whether to create extra test users defined by the test

        Returns:
            int: status code: 0 = success, 128 = failure

        """
        users = test.get_yaml_client_users()
        clients = test.host_info.clients.hosts
        if users:
            logger.info('Setting up test users on %s', clients)

        # Keep track of queried groups to avoid redundant work
        group_gid = {}

        # Query and optionally create all groups and users
        for _user in users:
            user, *group = _user.split(':')
            group = group[0] if group else None

            # Save the group's gid
            if group and group not in group_gid:
                try:
                    group_gid[group] = self._query_create_group(clients, group, create)
                except LaunchException as error:
                    self.test_result.fail_test("Prepare", str(error), sys.exc_info())
                    return 128

            gid = group_gid.get(group, None)
            try:
                self._query_create_user(clients, user, gid, create)
            except LaunchException as error:
                self.test_result.fail_test("Prepare", str(error), sys.exc_info())
                return 128

        return 0

    @staticmethod
    def _query_create_group(hosts, group, create=False):
        """Query and optionally create a group on remote hosts.

        Args:
            hosts (NodeSet): hosts on which to query and create the group
            group (str): group to query and create
            create (bool, optional): whether to create the group if non-existent

        Raises:
            LaunchException: if there is an error querying or creating the group

        Returns:
            str: the group's gid

        """
        # Get the group id on each node
        logger.info('Querying group %s', group)
        group_ids = get_group_id(hosts, group).keys()
        logger.debug('  found group_ids %s', group_ids)
        group_ids = list(group_ids)
        if len(group_ids) == 1 and group_ids[0] is not None:
            return group_ids[0]
        if not create:
            raise LaunchException(f'Group not setup correctly: {group}')

        # Create the group
        logger.info('Creating group %s', group)
        if not groupadd(hosts, group, True, True).passed:
            raise LaunchException(f'Error creating group {group}')

        # Get the group id on each node
        logger.info('Querying group %s', group)
        group_ids = get_group_id(hosts, group).keys()
        logger.debug('  found group_ids %s', group_ids)
        group_ids = list(group_ids)
        if len(group_ids) == 1 and group_ids[0] is not None:
            return group_ids[0]
        raise LaunchException(f'Group not setup correctly: {group}')

    @staticmethod
    def _query_create_user(hosts, user, gid=None, create=False):
        """Query and optionally create a user on remote hosts.

        Args:
            hosts (NodeSet): hosts on which to query and create the group
            user (str): user to query and create
            gid (str, optional): user's primary gid. Default is None
            create (bool, optional): whether to create the group if non-existent. Default is False

        Raises:
            LaunchException: if there is an error querying or creating the user

        """
        logger.info('Querying user %s', user)
        groups = get_user_groups(hosts, user)
        logger.debug('  found groups %s', groups)
        groups = list(groups)
        if len(groups) == 1 and groups[0] == gid:
            # Exists and in correct group
            return
        if not create:
            raise LaunchException(f'User {user} groups not as expected')

        # Delete and ignore errors, in case user account is inconsistent across nodes
        logger.info('Deleting user %s', user)
        _ = userdel(hosts, user, True)

        logger.info('Creating user %s in group %s', user, gid)
        test_env = TestEnvironment()
        if not useradd(hosts, user, gid, test_env.user_dir, True).passed:
            raise LaunchException(f'Error creating user {user}')

    def _generate_certs(self):
        """Generate the certificates for the test.

        Returns:
            int: status code: 0 = success, 128 = failure

        """
        logger.debug("-" * 80)
        logger.debug("Generating certificates")
        test_env = TestEnvironment()
        certs_dir = os.path.join(test_env.log_dir, "daosCA")
        certgen_dir = os.path.abspath(
            os.path.join("..", "..", "..", "..", "lib64", "daos", "certgen"))
        command = os.path.join(certgen_dir, "gen_certificates.sh")
        try:
            run_local(f"/usr/bin/rm -rf {certs_dir}")
            run_local(f"{command} {test_env.log_dir}")
        except RunException:
            message = "Error generating certificates"
            self.test_result.fail_test("Prepare", message, sys.exc_info())
            return 128
        return 0

    def _collect_crash_files(self):
        """Move any avocado crash files into job-results/latest/crashes."""
        avocado_logs_dir = self.avocado.get_logs_dir()
        crash_dir = os.path.join(avocado_logs_dir.replace("job-results", "data"), "crashes")
        if os.path.isdir(crash_dir):
            crash_files = [
                os.path.join(crash_dir, crash_file)
                for crash_file in os.listdir(crash_dir)
                if os.path.isfile(os.path.join(crash_dir, crash_file))]

            if crash_files:
                latest_crash_dir = os.path.join(avocado_logs_dir, "latest", "crashes")
                try:
                    run_local(f"mkdir -p {latest_crash_dir}", check=True)
                    for crash_file in crash_files:
                        run_local(f"mv {crash_file} {latest_crash_dir}", check=True)
                except RunException:
                    message = "Error collecting crash files"
                    self.test_result.fail_test("Execute", message, sys.exc_info())
            else:
                logger.debug("No avocado crash files found in %s", crash_dir)
